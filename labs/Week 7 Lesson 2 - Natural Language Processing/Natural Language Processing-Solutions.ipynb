{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Science Part Time Course - Sydney \n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "This notebook contains exercises for getting started with Natual Language Processing in Python. The main topics we will cover in this class are:\n",
    "\n",
    "Introduction\n",
    "1. newsgroups dataset\n",
    "- Bag of words prediciton model\n",
    "\n",
    "Advanced Language Processing with NLTK \n",
    "1. Tokenizing\n",
    "- Stemming\n",
    "- Speech Tagging\n",
    "- Named Entity Recognition\n",
    "- Term Frequency - Inverse Document Frequency\n",
    "- Latent Dirichlet Allocation\n",
    "- Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Prediction\n",
    "\n",
    "We will use the [20 Newsgroup dataset](http://qwone.com/~jason/20Newsgroups/), which is provided by Scikit-Learn. This is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "We will restrict the analysis to 4 groups and will attempt to classify them starting from the corresponding text.\n",
    "\n",
    "This is a typical example of text classification, where a data scientist's task is to train a model that can partition text in pre-defined categories. Other examples include sentiment analysis and topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 (of 20 available) categories of news articles to download\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "# retrieve the prepared train data\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# retrieve the prepared test data\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What data type is `data_train` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: how to you retrieve the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the .data method to retrive the data component form the train and test object\n",
    "data_train.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Inspect the first data point, what does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "What is CountVectoriser? \n",
    "See the following documentation. It is for counting occurances of words.\n",
    "\n",
    "[Text Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "Workflow we will follow:\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data (tokenization and occurrence counting)\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Logistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a standard CountVectorizer and fit the training data (tokenization and occurrence counting)\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intialise CountVectorizer and fit on the dataset\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: how big is the feature dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26879"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat the fit process, this time eliminating english stop words\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(data_train['data'])\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: is the dictionary smaller?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the training data using the trained vectorizer\n",
    "X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(),\n",
    "                       columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: what are the 20 words that are most common in the whole corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space       1061\n",
       "people       793\n",
       "god          745\n",
       "don          730\n",
       "like         682\n",
       "just         675\n",
       "does         600\n",
       "know         592\n",
       "think        584\n",
       "time         546\n",
       "image        534\n",
       "edu          501\n",
       "use          468\n",
       "good         449\n",
       "data         444\n",
       "nasa         419\n",
       "graphics     414\n",
       "jesus        411\n",
       "say          409\n",
       "way          387\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = X_train.sum(axis=0)\n",
    "word_counts.sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 target names\n",
    "names = data_train['target_names']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism most common words\n",
      "god         405\n",
      "people      330\n",
      "don         262\n",
      "think       215\n",
      "just        209\n",
      "does        207\n",
      "atheism     199\n",
      "say         174\n",
      "believe     163\n",
      "like        162\n",
      "atheists    162\n",
      "religion    156\n",
      "jesus       155\n",
      "know        154\n",
      "argument    148\n",
      "time        135\n",
      "said        131\n",
      "true        131\n",
      "bible       121\n",
      "way         120\n",
      "dtype: int64\n",
      "\n",
      "comp.graphics most common words\n",
      "image        484\n",
      "graphics     410\n",
      "edu          297\n",
      "jpeg         267\n",
      "file         265\n",
      "use          225\n",
      "data         219\n",
      "files        217\n",
      "images       212\n",
      "software     212\n",
      "program      199\n",
      "ftp          189\n",
      "available    185\n",
      "format       178\n",
      "color        174\n",
      "like         167\n",
      "know         165\n",
      "pub          161\n",
      "gif          160\n",
      "does         157\n",
      "dtype: int64\n",
      "\n",
      "sci.space most common words\n",
      "space        989\n",
      "nasa         374\n",
      "launch       267\n",
      "earth        222\n",
      "like         222\n",
      "data         216\n",
      "orbit        201\n",
      "time         197\n",
      "shuttle      192\n",
      "just         189\n",
      "satellite    187\n",
      "lunar        182\n",
      "moon         168\n",
      "new          158\n",
      "program      156\n",
      "don          151\n",
      "year         146\n",
      "people       142\n",
      "mission      141\n",
      "use          134\n",
      "dtype: int64\n",
      "\n",
      "talk.religion.misc most common words\n",
      "god          329\n",
      "people       267\n",
      "jesus        256\n",
      "don          162\n",
      "bible        160\n",
      "just         159\n",
      "think        151\n",
      "christian    151\n",
      "say          149\n",
      "know         149\n",
      "does         147\n",
      "did          132\n",
      "like         131\n",
      "good         131\n",
      "life         118\n",
      "way          118\n",
      "believe      117\n",
      "said         103\n",
      "point        101\n",
      "time          99\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# what are the 20 most common words in each of the 4 classes?\n",
    "\n",
    "common_words = []\n",
    "for i in range(4):\n",
    "    word_count = X_train[y_train==i].sum(axis=0)\n",
    "    print(names[i], \"most common words\")\n",
    "    cw = word_count.sort_values(ascending = False).head(20)\n",
    "#     cw.to_csv('../../../5.2-lesson/assets/datasets/'+names[i]+'_most_common_words.csv')\n",
    "    print(cw)\n",
    "    common_words.extend(cw.index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of a Logistic Regression on the features extracted by the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(),\n",
    "                      columns=cvec.get_feature_names())\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74501108647450109"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try the following 3 modification:\n",
    "\n",
    "- restrict the max_features\n",
    "- change max_df and min_df\n",
    "- use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict the max_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69696969697\n",
      "Number of features: 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=1000),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "docm(y_test, y_pred, names)\n",
    "print(\"Number of features:\", len(model.steps[0][1].get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a confusion matrix and investigate what gets mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if labels is not None:\n",
    "        cols = ['p_'+c for c in labels]\n",
    "        df = pd.DataFrame(cm, index=labels, columns=cols)\n",
    "    else:\n",
    "        cols = ['p_'+str(i) for i in xrange(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_alt.atheism</th>\n",
       "      <th>p_comp.graphics</th>\n",
       "      <th>p_sci.space</th>\n",
       "      <th>p_talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>172</td>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>16</td>\n",
       "      <td>331</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>312</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>76</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    p_alt.atheism  p_comp.graphics  p_sci.space  \\\n",
       "alt.atheism                   172               15           59   \n",
       "comp.graphics                  16              331           36   \n",
       "sci.space                      28               29          312   \n",
       "talk.religion.misc             76               16           31   \n",
       "\n",
       "                    p_talk.religion.misc  \n",
       "alt.atheism                           73  \n",
       "comp.graphics                          6  \n",
       "sci.space                             25  \n",
       "talk.religion.misc                   128  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docm(y_test, y_pred, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change max_df and min_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70436067997\n",
      "Number of features: 1258\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      max_features=2000,\n",
    "                                      min_df=0.01),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "docm(y_test, y_pred, names)\n",
    "print(\"Number of features:\", len(model.steps[0][1].get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_alt.atheism</th>\n",
       "      <th>p_comp.graphics</th>\n",
       "      <th>p_sci.space</th>\n",
       "      <th>p_talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>167</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>14</td>\n",
       "      <td>328</td>\n",
       "      <td>39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>68</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    p_alt.atheism  p_comp.graphics  p_sci.space  \\\n",
       "alt.atheism                   167               15           58   \n",
       "comp.graphics                  14              328           39   \n",
       "sci.space                      23               26          320   \n",
       "talk.religion.misc             68               18           27   \n",
       "\n",
       "                    p_talk.religion.misc  \n",
       "alt.atheism                           79  \n",
       "comp.graphics                          8  \n",
       "sci.space                             25  \n",
       "talk.religion.misc                   138  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docm(y_test, y_pred, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589061345159\n",
      "Number of features: 54\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(CountVectorizer(stop_words='english',\n",
    "                                      vocabulary=set(common_words)),\n",
    "                      LogisticRegression(),\n",
    "                      )\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_pred = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "docm(y_test, y_pred, names)\n",
    "print(\"Number of features:\", len(model.steps[0][1].get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_alt.atheism</th>\n",
       "      <th>p_comp.graphics</th>\n",
       "      <th>p_sci.space</th>\n",
       "      <th>p_talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>160</td>\n",
       "      <td>67</td>\n",
       "      <td>33</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>25</td>\n",
       "      <td>314</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>247</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    p_alt.atheism  p_comp.graphics  p_sci.space  \\\n",
       "alt.atheism                   160               67           33   \n",
       "comp.graphics                  25              314           44   \n",
       "sci.space                      37               88          247   \n",
       "talk.religion.misc             90               70           15   \n",
       "\n",
       "                    p_talk.religion.misc  \n",
       "alt.atheism                           59  \n",
       "comp.graphics                          6  \n",
       "sci.space                             22  \n",
       "talk.religion.misc                    76  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docm(y_test, y_pred, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not downloaded NLTK, use the following method to download just the essentials:\n",
    "\n",
    "For the following step, see the new window that will pop out elsewhere. The whole NLTK package is huge. Too big for all of you to download in class at the same time. Instead, select to install just the main packages and see of the lab runs. If not, use the downloader to get the additional missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find pop-up window for downloader tool\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "\n",
    "Why:   Gives structure to previously unstructured text\n",
    "\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "\n",
    "\n",
    "\"corpus\" = collection of documents\n",
    "\n",
    "\"corpora\" = plural form of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data science, also known as data-driven science, is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] similar to data mining. Data science is a \"concept to unify statistics, data analysis and their related methods\" in order to \"understand and analyze actual phenomena\" with data.[3] It employs techniques and theories drawn from many fields within the broad areas of mathe'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
    "b = BeautifulSoup(r.text, \"lxml\")\n",
    "paragraphs = b.find(\"body\").findAll(\"p\")\n",
    "text = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text + \" \"\n",
    "# Data Science corpus\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science, also known as data-driven science, is an interdisciplinary field about scientific methods, processes, and systems to extract knowledge or insights from data in various forms, either structured or unstructured,[1][2] similar to data mining.',\n",
       " 'Data science is a \"concept to unify statistics, data analysis and their related methods\" in order to \"understand and analyze actual phenomena\" with data.',\n",
       " '[3] It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science, in particular from the subdomains of machine learning, classification, cluster analysis, data mining, databases, and visualization.',\n",
       " 'Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.',\n",
       " '[4][5] When Harvard Business Review called it \"The Sexiest Job of the 21st Century\" [6] the term became a buzzword, and is now often applied to business analytics,[7] or even arbitrary use of data, or used as a sexed-up term for statistics.',\n",
       " '[8] While many university programs now offer a data science degree, there exists no consensus on a definition or curriculum contents.',\n",
       " '[7] Because of the current popularity of this term, there are many \"advocacy efforts\" surrounding it.',\n",
       " '[9]   The term \"data science\" (originally used interchangeably with \"datalogy\") has existed for over thirty years and was used initially as a substitute for computer science by Peter Naur in 1960.',\n",
       " 'In 1974, Naur published Concise Survey of Computer Methods, which freely used the term data science in its survey of the contemporary data processing methods that are used in a wide range of applications.',\n",
       " 'In 1996, members of the International Federation of Classification Societies (IFCS) met in Kobe for their biennial conference.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into sentences\n",
    "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " ',',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'data-driven',\n",
       " 'science',\n",
       " ',',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'about',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " ',',\n",
       " 'processes',\n",
       " ',',\n",
       " 'and',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'knowledge',\n",
       " 'or',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'data',\n",
       " 'in',\n",
       " 'various',\n",
       " 'forms',\n",
       " ',',\n",
       " 'either',\n",
       " 'structured',\n",
       " 'or',\n",
       " 'unstructured',\n",
       " ',',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'data',\n",
       " 'mining',\n",
       " '.',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'a',\n",
       " '``',\n",
       " 'concept',\n",
       " 'to',\n",
       " 'unify',\n",
       " 'statistics',\n",
       " ',',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'their',\n",
       " 'related',\n",
       " 'methods',\n",
       " \"''\",\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " '``',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'actual',\n",
       " 'phenomena',\n",
       " \"''\",\n",
       " 'with',\n",
       " 'data',\n",
       " '.',\n",
       " '[',\n",
       " '3',\n",
       " ']',\n",
       " 'It',\n",
       " 'employs',\n",
       " 'techniques',\n",
       " 'and',\n",
       " 'theories',\n",
       " 'drawn',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields',\n",
       " 'within',\n",
       " 'the',\n",
       " 'broad',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'mathematics',\n",
       " ',',\n",
       " 'statistics',\n",
       " ',',\n",
       " 'information']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = [word for word in nltk.word_tokenize(text)]\n",
    "tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " 'science',\n",
       " 'also',\n",
       " 'known',\n",
       " 'as',\n",
       " 'data-driven',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'about',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'processes',\n",
       " 'and',\n",
       " 'systems',\n",
       " 'to',\n",
       " 'extract',\n",
       " 'knowledge',\n",
       " 'or',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'data',\n",
       " 'in',\n",
       " 'various',\n",
       " 'forms',\n",
       " 'either',\n",
       " 'structured',\n",
       " 'or',\n",
       " 'unstructured',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'data',\n",
       " 'mining',\n",
       " 'Data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'a',\n",
       " 'concept',\n",
       " 'to',\n",
       " 'unify',\n",
       " 'statistics',\n",
       " 'data',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'their',\n",
       " 'related',\n",
       " 'methods',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'actual',\n",
       " 'phenomena',\n",
       " 'with',\n",
       " 'data',\n",
       " 'It',\n",
       " 'employs',\n",
       " 'techniques',\n",
       " 'and',\n",
       " 'theories',\n",
       " 'drawn',\n",
       " 'from',\n",
       " 'many',\n",
       " 'fields',\n",
       " 'within',\n",
       " 'the',\n",
       " 'broad',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'mathematics',\n",
       " 'statistics',\n",
       " 'information',\n",
       " 'science',\n",
       " 'and',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'from',\n",
       " 'the',\n",
       " 'subdomains',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'classification',\n",
       " 'cluster',\n",
       " 'analysis',\n",
       " 'data',\n",
       " 'mining',\n",
       " 'databases',\n",
       " 'and',\n",
       " 'visualization',\n",
       " 'Turing',\n",
       " 'award',\n",
       " 'winner',\n",
       " 'Jim']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep tokens that start with a letter (using regular expressions)\n",
    "import re\n",
    "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
    "clean_tokens[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 59),\n",
       " ('and', 57),\n",
       " ('data', 45),\n",
       " ('of', 45),\n",
       " ('in', 27),\n",
       " ('science', 26),\n",
       " ('Data', 25),\n",
       " ('a', 25),\n",
       " ('to', 23),\n",
       " ('In', 17),\n",
       " ('Science', 16),\n",
       " ('for', 13),\n",
       " ('is', 12),\n",
       " ('term', 12),\n",
       " ('as', 11),\n",
       " ('The', 10),\n",
       " ('on', 10),\n",
       " ('was', 8),\n",
       " ('Statistical', 8),\n",
       " ('with', 7),\n",
       " ('his', 7),\n",
       " ('software', 7),\n",
       " ('methods', 6),\n",
       " ('statistics', 6),\n",
       " ('their', 6)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the tokens\n",
    "from collections import Counter\n",
    "c = Counter(clean_tokens)\n",
    "\n",
    "c.most_common(25)       # mixed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASA 1\n",
      "Action 1\n",
      "Advanced 2\n",
      "Although 1\n",
      "American 2\n",
      "An 1\n",
      "Analysis 2\n",
      "Analytics 3\n",
      "April 2\n",
      "Areas 1\n",
      "Assembly 1\n",
      "Association 3\n",
      "August 1\n",
      "Because 1\n",
      "Board 1\n",
      "Business 3\n",
      "C. 1\n",
      "C.F 1\n",
      "CODATA 1\n",
      "Carver 1\n",
      "Century 4\n",
      "Chandra 1\n",
      "Chikio 1\n",
      "Classification 1\n",
      "Cleveland 2\n"
     ]
    }
   ],
   "source": [
    "sorted(c.items())[:25]  # counts similar words separately\n",
    "\n",
    "#print\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###################\n",
    "##### EXERCISE ####\n",
    "###################\n",
    "\n",
    "- Put each word in clean_tokens in lower case\n",
    "- find the new word count of the lowered tokens\n",
    "- Then show the top 10 words used in this corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put each word in clean_tokens in lower case\n",
    "clean_tokens_lower = [token.lower() for token in clean_tokens]\n",
    "clean_tokens_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 25),\n",
       " ('ability', 1),\n",
       " ('about', 2),\n",
       " ('academics', 1),\n",
       " ('action', 1),\n",
       " ('activity', 1),\n",
       " ('actual', 1),\n",
       " ('added', 1),\n",
       " ('address', 1),\n",
       " ('advanced', 2),\n",
       " ('advances', 1),\n",
       " ('advocacy', 1),\n",
       " ('advocated', 1),\n",
       " ('after', 1),\n",
       " ('aid', 1),\n",
       " ('algorithms', 1),\n",
       " ('all', 1),\n",
       " ('allows', 1),\n",
       " ('also', 2),\n",
       " ('although', 1),\n",
       " ('american', 2),\n",
       " ('amounts', 1),\n",
       " ('an', 5),\n",
       " ('analysis', 7),\n",
       " ('analytical', 1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the new word count of the lowered tokens\n",
    "c = Counter(clean_tokens_lower)\n",
    "sorted(c.items())[:25]  # counts similar words separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 70),\n",
       " ('the', 69),\n",
       " ('and', 57),\n",
       " ('of', 45),\n",
       " ('in', 44),\n",
       " ('science', 42),\n",
       " ('a', 25),\n",
       " ('to', 23),\n",
       " ('for', 13),\n",
       " ('is', 12)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then show the top 10 words used in this corpus\n",
    "c.most_common(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "\n",
    "Notes: \n",
    "\n",
    "- Uses a \"simple\" and fast rule-based approach\n",
    "- Output can be undesirable for irregular words\n",
    "- Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "- Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charg'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example stemming\n",
    "stemmer.stem('charge')\n",
    "stemmer.stem('charging')\n",
    "stemmer.stem('charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 25),\n",
       " ('abil', 1),\n",
       " ('about', 2),\n",
       " ('academ', 1),\n",
       " ('action', 1),\n",
       " ('activ', 1),\n",
       " ('actual', 1),\n",
       " ('ad', 1),\n",
       " ('address', 1),\n",
       " ('advanc', 3),\n",
       " ('advoc', 1),\n",
       " ('advocaci', 1),\n",
       " ('after', 1),\n",
       " ('aid', 1),\n",
       " ('algorithm', 1),\n",
       " ('all', 1),\n",
       " ('allow', 1),\n",
       " ('also', 2),\n",
       " ('although', 1),\n",
       " ('american', 2),\n",
       " ('amount', 1),\n",
       " ('an', 5),\n",
       " ('analysi', 7),\n",
       " ('analyt', 6),\n",
       " ('analytics”', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the stemmed tokens\n",
    "c = Counter(stemmed_tokens)\n",
    "c.most_common(25)       # all lowercase\n",
    "sorted(c.items())[:25]  # some are strange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "    \n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "    \n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compare stemmer to lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare stemmer to lemmatizer\n",
    "stemmer.stem('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'wolves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wolv'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('wolves') # Beter for information retrieval and search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wolf'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('wolves') # Better for text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('is',pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "What:  Determine the part of speech of a word\n",
    "    \n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "    \n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('revision', 'NN'),\n",
       " ('class', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('Saturday', 'NNP')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_sent = 'We have a revision class this Saturday'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "What:  Remove common words that will likely appear in any text\n",
    "    \n",
    "Why:   They don't tell you much about your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 70),\n",
       " ('the', 69),\n",
       " ('and', 57),\n",
       " ('of', 45),\n",
       " ('in', 44),\n",
       " ('scienc', 42),\n",
       " ('a', 25),\n",
       " ('to', 23),\n",
       " ('statist', 19),\n",
       " ('for', 13),\n",
       " ('is', 12),\n",
       " ('term', 12),\n",
       " ('as', 11),\n",
       " ('scientist', 11),\n",
       " ('use', 10),\n",
       " ('on', 10),\n",
       " ('it', 8),\n",
       " ('was', 8),\n",
       " ('method', 7),\n",
       " ('analysi', 7),\n",
       " ('with', 7),\n",
       " ('comput', 7),\n",
       " ('his', 7),\n",
       " ('softwar', 7),\n",
       " ('their', 6)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##################\n",
    "### Exercise  ####\n",
    "##################\n",
    "\n",
    "\n",
    "- Create a variable called stemmed_stops which is the stemmed version of each stopword in stopwords. Use the stemmer we used up above!\n",
    "- Then create a list called stemmed_tokens_no_stop that contains only the tokens in stemmed_tokens that aren't in stemmed_stops\n",
    "- Show the 25 most common stemmed non stop word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variable called stemmed_stops which is the stemmed version of each stopword in stopwords\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'our',\n",
       " 'ourselv',\n",
       " 'you',\n",
       " 'your',\n",
       " 'your',\n",
       " 'yourself',\n",
       " 'yourselv',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'her',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'their',\n",
       " 'themselv',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'be',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'have',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'did',\n",
       " 'do',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'becaus',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'dure',\n",
       " 'befor',\n",
       " 'after',\n",
       " 'abov',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'onc',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whi',\n",
       " 'how',\n",
       " 'all',\n",
       " 'ani',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'onli',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'veri',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_stops = [stemmer.stem(words) for words in stopwords]\n",
    "stemmed_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'also',\n",
       " 'known',\n",
       " 'data-driven',\n",
       " 'scienc',\n",
       " 'interdisciplinari',\n",
       " 'field',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'process',\n",
       " 'system',\n",
       " 'extract',\n",
       " 'knowledg',\n",
       " 'insight',\n",
       " 'data',\n",
       " 'various',\n",
       " 'form',\n",
       " 'either',\n",
       " 'structur',\n",
       " 'unstructur',\n",
       " 'similar',\n",
       " 'data',\n",
       " 'mine',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'concept',\n",
       " 'unifi',\n",
       " 'statist',\n",
       " 'data',\n",
       " 'analysi',\n",
       " 'relat',\n",
       " 'method',\n",
       " 'order',\n",
       " 'understand',\n",
       " 'analyz',\n",
       " 'actual',\n",
       " 'phenomena',\n",
       " 'data',\n",
       " 'employ',\n",
       " 'techniqu',\n",
       " 'theori',\n",
       " 'drawn',\n",
       " 'mani',\n",
       " 'field',\n",
       " 'within',\n",
       " 'broad',\n",
       " 'area',\n",
       " 'mathemat',\n",
       " 'statist',\n",
       " 'inform',\n",
       " 'scienc',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'particular',\n",
       " 'subdomain',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'classif',\n",
       " 'cluster',\n",
       " 'analysi',\n",
       " 'data',\n",
       " 'mine',\n",
       " 'databa',\n",
       " 'visual',\n",
       " 'ture',\n",
       " 'award',\n",
       " 'winner',\n",
       " 'jim',\n",
       " 'gray',\n",
       " 'imagin',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'fourth',\n",
       " 'paradigm',\n",
       " 'scienc',\n",
       " 'empir',\n",
       " 'theoret',\n",
       " 'comput',\n",
       " 'data-driven',\n",
       " 'assert',\n",
       " 'everyth',\n",
       " 'scienc',\n",
       " 'chang',\n",
       " 'impact',\n",
       " 'inform',\n",
       " 'technolog',\n",
       " 'data',\n",
       " 'delug',\n",
       " 'harvard',\n",
       " 'busi',\n",
       " 'review',\n",
       " 'call',\n",
       " 'sexiest',\n",
       " 'job',\n",
       " 'centuri',\n",
       " 'term',\n",
       " 'becam',\n",
       " 'buzzword',\n",
       " 'often',\n",
       " 'appli',\n",
       " 'busi',\n",
       " 'analyt',\n",
       " 'even',\n",
       " 'arbitrari',\n",
       " 'use',\n",
       " 'data',\n",
       " 'use',\n",
       " 'sexed-up',\n",
       " 'term',\n",
       " 'statist',\n",
       " 'mani',\n",
       " 'univ',\n",
       " 'program',\n",
       " 'offer',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'degr',\n",
       " 'exist',\n",
       " 'consensus',\n",
       " 'definit',\n",
       " 'curriculum',\n",
       " 'content',\n",
       " 'current',\n",
       " 'popular',\n",
       " 'term',\n",
       " 'mani',\n",
       " 'advocaci',\n",
       " 'effort',\n",
       " 'surround',\n",
       " 'term',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'origin',\n",
       " 'use',\n",
       " 'interchang',\n",
       " 'datalog',\n",
       " 'exist',\n",
       " 'thirti',\n",
       " 'year',\n",
       " 'use',\n",
       " 'initi',\n",
       " 'substitut',\n",
       " 'comput',\n",
       " 'scienc',\n",
       " 'peter',\n",
       " 'naur',\n",
       " 'naur',\n",
       " 'publish',\n",
       " 'conci',\n",
       " 'survey',\n",
       " 'comput',\n",
       " 'method',\n",
       " 'freeli',\n",
       " 'use',\n",
       " 'term',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'survey',\n",
       " 'contemporari',\n",
       " 'data',\n",
       " 'process',\n",
       " 'method',\n",
       " 'use',\n",
       " 'wide',\n",
       " 'rang',\n",
       " 'applic',\n",
       " 'member',\n",
       " 'intern',\n",
       " 'feder',\n",
       " 'classif',\n",
       " 'societi',\n",
       " 'ifc',\n",
       " 'met',\n",
       " 'kobe',\n",
       " 'biennial',\n",
       " 'confer',\n",
       " 'first',\n",
       " 'time',\n",
       " 'term',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'includ',\n",
       " 'titl',\n",
       " 'confer',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'classif',\n",
       " 'relat',\n",
       " 'method',\n",
       " 'term',\n",
       " 'introduc',\n",
       " 'roundtabl',\n",
       " 'discuss',\n",
       " 'chikio',\n",
       " 'hayashi',\n",
       " 'novemb',\n",
       " 'c.f',\n",
       " 'jeff',\n",
       " 'wu',\n",
       " 'gave',\n",
       " 'inaugur',\n",
       " 'lectur',\n",
       " 'entitl',\n",
       " 'statist',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'appoint',\n",
       " 'h.',\n",
       " 'c.',\n",
       " 'carver',\n",
       " 'professorship',\n",
       " 'univ',\n",
       " 'michigan',\n",
       " 'lectur',\n",
       " 'charact',\n",
       " 'statist',\n",
       " 'work',\n",
       " 'trilog',\n",
       " 'data',\n",
       " 'collect',\n",
       " 'data',\n",
       " 'model',\n",
       " 'analysi',\n",
       " 'deci',\n",
       " 'make',\n",
       " 'conclus',\n",
       " 'initi',\n",
       " 'modern',\n",
       " 'non-comput',\n",
       " 'scienc',\n",
       " 'usag',\n",
       " 'term',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'advoc',\n",
       " 'statist',\n",
       " 'renam',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'statistician',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'later',\n",
       " 'present',\n",
       " 'lectur',\n",
       " 'entitl',\n",
       " 'statist',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'first',\n",
       " 'p.c',\n",
       " 'mahalanobi',\n",
       " 'memori',\n",
       " 'lectur',\n",
       " 'lectur',\n",
       " 'honor',\n",
       " 'prasanta',\n",
       " 'chandra',\n",
       " 'mahalanobi',\n",
       " 'indian',\n",
       " 'scientist',\n",
       " 'statistician',\n",
       " 'founder',\n",
       " 'indian',\n",
       " 'statist',\n",
       " 'institut',\n",
       " 'william',\n",
       " 's.',\n",
       " 'cleveland',\n",
       " 'introduc',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'independ',\n",
       " 'disciplin',\n",
       " 'extend',\n",
       " 'field',\n",
       " 'statist',\n",
       " 'incorpor',\n",
       " 'advanc',\n",
       " 'comput',\n",
       " 'data',\n",
       " 'articl',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'action',\n",
       " 'plan',\n",
       " 'expand',\n",
       " 'technic',\n",
       " 'area',\n",
       " 'field',\n",
       " 'statist',\n",
       " 'publish',\n",
       " 'volum',\n",
       " 'april',\n",
       " 'edit',\n",
       " 'intern',\n",
       " 'statist',\n",
       " 'review',\n",
       " 'revu',\n",
       " 'intern',\n",
       " 'de',\n",
       " 'statistiqu',\n",
       " 'report',\n",
       " 'cleveland',\n",
       " 'establish',\n",
       " 'six',\n",
       " 'technic',\n",
       " 'area',\n",
       " 'believ',\n",
       " 'encompass',\n",
       " 'field',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'multidisciplinari',\n",
       " 'investig',\n",
       " 'model',\n",
       " 'method',\n",
       " 'data',\n",
       " 'comput',\n",
       " 'data',\n",
       " 'pedagogi',\n",
       " 'tool',\n",
       " 'evalu',\n",
       " 'theori',\n",
       " 'april',\n",
       " 'intern',\n",
       " 'council',\n",
       " 'scienc',\n",
       " 'committ',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'technolog',\n",
       " 'codata',\n",
       " 'start',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'journal',\n",
       " 'public',\n",
       " 'focus',\n",
       " 'issu',\n",
       " 'descript',\n",
       " 'data',\n",
       " 'system',\n",
       " 'public',\n",
       " 'internet',\n",
       " 'applic',\n",
       " 'legal',\n",
       " 'issu',\n",
       " 'short',\n",
       " 'thereaft',\n",
       " 'januari',\n",
       " 'columbia',\n",
       " 'univ',\n",
       " 'began',\n",
       " 'publish',\n",
       " 'journal',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'provid',\n",
       " 'platform',\n",
       " 'data',\n",
       " 'worker',\n",
       " 'present',\n",
       " 'view',\n",
       " 'exchang',\n",
       " 'idea',\n",
       " 'journal',\n",
       " 'larg',\n",
       " 'devot',\n",
       " 'applic',\n",
       " 'statist',\n",
       " 'method',\n",
       " 'quantit',\n",
       " 'research',\n",
       " 'nation',\n",
       " 'scienc',\n",
       " 'board',\n",
       " 'publish',\n",
       " 'long-liv',\n",
       " 'digit',\n",
       " 'data',\n",
       " 'collect',\n",
       " 'enabl',\n",
       " 'research',\n",
       " 'educ',\n",
       " 'centuri',\n",
       " 'defin',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'inform',\n",
       " 'comput',\n",
       " 'scientist',\n",
       " 'databa',\n",
       " 'softwar',\n",
       " 'programm',\n",
       " 'disciplinari',\n",
       " 'expert',\n",
       " 'curat',\n",
       " 'expert',\n",
       " 'annot',\n",
       " 'librarian',\n",
       " 'archivist',\n",
       " 'crucial',\n",
       " 'success',\n",
       " 'manag',\n",
       " 'digit',\n",
       " 'data',\n",
       " 'collect',\n",
       " 'whose',\n",
       " 'primari',\n",
       " 'activ',\n",
       " 'conduct',\n",
       " 'creativ',\n",
       " 'inquiri',\n",
       " 'analysi',\n",
       " 'harvard',\n",
       " 'busi',\n",
       " 'review',\n",
       " 'articl',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'sexiest',\n",
       " 'job',\n",
       " 'centuri',\n",
       " 'dj',\n",
       " 'patil',\n",
       " 'claim',\n",
       " 'coin',\n",
       " 'term',\n",
       " 'jeff',\n",
       " 'hammerbach',\n",
       " 'defin',\n",
       " 'job',\n",
       " 'linkedin',\n",
       " 'facebook',\n",
       " 'respect',\n",
       " 'assert',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'new',\n",
       " 'breed',\n",
       " 'shortag',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'becom',\n",
       " 'serious',\n",
       " 'constraint',\n",
       " 'sector',\n",
       " 'describ',\n",
       " 'much',\n",
       " 'busi',\n",
       " 'orient',\n",
       " 'role',\n",
       " 'ieee',\n",
       " 'task',\n",
       " 'forc',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'advanc',\n",
       " 'analyt',\n",
       " 'launch',\n",
       " 'first',\n",
       " 'intern',\n",
       " 'confer',\n",
       " 'ieee',\n",
       " 'intern',\n",
       " 'confer',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'advanc',\n",
       " 'analyt',\n",
       " 'launch',\n",
       " 'american',\n",
       " 'statist',\n",
       " 'associ',\n",
       " 'section',\n",
       " 'statist',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'mine',\n",
       " 'renam',\n",
       " 'journal',\n",
       " 'statist',\n",
       " 'analysi',\n",
       " 'data',\n",
       " 'mine',\n",
       " 'asa',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'journal',\n",
       " 'chang',\n",
       " 'section',\n",
       " 'name',\n",
       " 'statist',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'intern',\n",
       " 'journal',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'analyt',\n",
       " 'launch',\n",
       " 'springer',\n",
       " 'publish',\n",
       " 'origin',\n",
       " 'work',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'big',\n",
       " 'data',\n",
       " 'analyt',\n",
       " 'first',\n",
       " 'european',\n",
       " 'confer',\n",
       " 'data',\n",
       " 'analysi',\n",
       " 'ecda',\n",
       " 'organi',\n",
       " 'luxembourg',\n",
       " 'establish',\n",
       " 'european',\n",
       " 'associ',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'euad',\n",
       " 'august',\n",
       " 'septemb',\n",
       " 'gesellschaft',\n",
       " 'für',\n",
       " 'klassifik',\n",
       " 'gfkl',\n",
       " 'ad',\n",
       " 'name',\n",
       " 'societi',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'societi',\n",
       " 'third',\n",
       " 'ecda',\n",
       " 'confer',\n",
       " 'univ',\n",
       " 'essex',\n",
       " 'colchest',\n",
       " 'uk',\n",
       " 'although',\n",
       " 'use',\n",
       " 'term',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'explod',\n",
       " 'busi',\n",
       " 'environ',\n",
       " 'mani',\n",
       " 'academ',\n",
       " 'journalist',\n",
       " 'see',\n",
       " 'distinct',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'statist',\n",
       " 'write',\n",
       " 'forb',\n",
       " 'gil',\n",
       " 'press',\n",
       " 'argu',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'buzzword',\n",
       " 'without',\n",
       " 'clear',\n",
       " 'definit',\n",
       " 'simpli',\n",
       " 'replac',\n",
       " 'analytics”',\n",
       " 'context',\n",
       " 'graduat',\n",
       " 'degr',\n",
       " 'program',\n",
       " 'question-and-answ',\n",
       " 'section',\n",
       " 'keynot',\n",
       " 'address',\n",
       " 'joint',\n",
       " 'statist',\n",
       " 'meet',\n",
       " 'american',\n",
       " 'statist',\n",
       " 'associ',\n",
       " 'note',\n",
       " 'appli',\n",
       " 'statistician',\n",
       " 'nate',\n",
       " 'silver',\n",
       " 'said',\n",
       " 'think',\n",
       " 'data-scientist',\n",
       " 'sex',\n",
       " 'term',\n",
       " 'statistician',\n",
       " 'branch',\n",
       " 'scienc',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'slight',\n",
       " 'redund',\n",
       " 'way',\n",
       " 'peopl',\n",
       " \"shouldn't\",\n",
       " 'berat',\n",
       " 'term',\n",
       " 'statistician.”',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'use',\n",
       " 'data',\n",
       " 'analyt',\n",
       " 'abil',\n",
       " 'find',\n",
       " 'interpret',\n",
       " 'rich',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'manag',\n",
       " 'larg',\n",
       " 'amount',\n",
       " 'data',\n",
       " 'despit',\n",
       " 'hardwar',\n",
       " 'softwar',\n",
       " 'bandwidth',\n",
       " 'constraint',\n",
       " 'merg',\n",
       " 'data',\n",
       " 'sourc',\n",
       " 'ensur',\n",
       " 'consist',\n",
       " 'dataset',\n",
       " 'creat',\n",
       " 'visual',\n",
       " 'aid',\n",
       " 'understand',\n",
       " 'data',\n",
       " 'build',\n",
       " 'mathemat',\n",
       " 'model',\n",
       " 'use',\n",
       " 'data',\n",
       " 'present',\n",
       " 'communic',\n",
       " 'data',\n",
       " 'insights/find',\n",
       " 'often',\n",
       " 'expect',\n",
       " 'produc',\n",
       " 'answer',\n",
       " 'day',\n",
       " 'rather',\n",
       " 'month',\n",
       " 'work',\n",
       " 'exploratori',\n",
       " 'analysi',\n",
       " 'rapid',\n",
       " 'iter',\n",
       " 'produc',\n",
       " 'present',\n",
       " 'result',\n",
       " 'dashboard',\n",
       " 'display',\n",
       " 'current',\n",
       " 'valu',\n",
       " 'rather',\n",
       " 'papers/report',\n",
       " 'statistician',\n",
       " 'normal',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'becom',\n",
       " 'popular',\n",
       " 'occup',\n",
       " 'harvard',\n",
       " 'busi',\n",
       " 'review',\n",
       " 'dub',\n",
       " 'sexiest',\n",
       " 'job',\n",
       " 'centuri',\n",
       " 'mckinsey',\n",
       " 'compani',\n",
       " 'project',\n",
       " 'global',\n",
       " 'excess',\n",
       " 'demand',\n",
       " 'million',\n",
       " 'new',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'univ',\n",
       " 'offer',\n",
       " 'master',\n",
       " 'cour',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'shorter',\n",
       " 'privat',\n",
       " 'bootcamp',\n",
       " 'also',\n",
       " 'offer',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'certif',\n",
       " 'includ',\n",
       " 'student-paid',\n",
       " 'program',\n",
       " 'like',\n",
       " 'general',\n",
       " 'assembl',\n",
       " 'employer-paid',\n",
       " 'program',\n",
       " 'like',\n",
       " 'data',\n",
       " 'incub',\n",
       " 'time',\n",
       " 'frame',\n",
       " 'data',\n",
       " 'scienc',\n",
       " 'softwar',\n",
       " 'reach',\n",
       " 'inflect',\n",
       " 'point',\n",
       " 'open',\n",
       " 'sourc',\n",
       " 'softwar',\n",
       " 'start',\n",
       " 'supplant',\n",
       " 'proprietari',\n",
       " 'softwar',\n",
       " 'use',\n",
       " 'open',\n",
       " 'sourc',\n",
       " 'softwar',\n",
       " 'enabl',\n",
       " 'modifi',\n",
       " 'extend',\n",
       " 'softwar',\n",
       " 'allow',\n",
       " 'share',\n",
       " 'result',\n",
       " 'algorithm']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list called stemmed_tokens_no_stop that contains only the tokens in stemmed_tokens that aren't in stemmed_stops\n",
    "stemmed_tokens_no_stop = [stemmer.stem(words) for words in stemmed_tokens if words not in stemmed_stops]\n",
    "stemmed_tokens_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', 70),\n",
       " ('scienc', 42),\n",
       " ('statist', 19),\n",
       " ('term', 12),\n",
       " ('scientist', 11),\n",
       " ('use', 10),\n",
       " ('method', 7),\n",
       " ('analysi', 7),\n",
       " ('comput', 7),\n",
       " ('intern', 7),\n",
       " ('softwar', 7),\n",
       " ('busi', 6),\n",
       " ('analyt', 6),\n",
       " ('confer', 6),\n",
       " ('journal', 6),\n",
       " ('field', 5),\n",
       " ('univ', 5),\n",
       " ('publish', 5),\n",
       " ('lectur', 5),\n",
       " ('statistician', 5),\n",
       " ('mine', 4),\n",
       " ('mani', 4),\n",
       " ('review', 4),\n",
       " ('job', 4),\n",
       " ('centuri', 4)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the 25 most common stemmed non stop word tokens\n",
    "\n",
    "c = Counter(stemmed_tokens_no_stop)\n",
    "c.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "\n",
    "Why:   Can help you to identify \"important\" words\n",
    "\n",
    "Notes: \n",
    "\n",
    "- Training NER classifier requires a lot of annotated training data\n",
    "- Should be trained on data relevant to your task\n",
    "- Stanford NER classifier is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alasdair', 'is', 'an', 'instructor', 'for', 'General', 'Assembly']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Alasdair is an instructor for General Assembly'\n",
    "\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alasdair', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('instructor', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('General', 'NNP'),\n",
       " ('Assembly', 'NNP')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "tagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqMAAABlCAIAAAD2032/AAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAHXRFWHRTb2Z0d2FyZQBHUEwgR2hvc3RzY3JpcHQgOS4xMJremEEAABhpSURBVHic7Z3Pj+TGdcdrtSvZ2hWiUPEObAfxDLhBAswARgJqYMAX6VB9WV3FRg4BVoYBNuCzAfJP6Ib9DzQvXh3TNHzTXsiDdYynCSRAZmAE6MJMBDnxTtIFWZ7xQtK6c3g7tSWyWc1m/2Kzv5/DgM0ii4+vyPfqvari3JpMJgwAAAAADeWVTQsAAAAAgBUCTw8AAAA0GXh6AAAAoMnc2bQAAGwrQgghBGPMcRzGmGVZm5YIAACmcAsz8gCoQBiGSZK4riulTJJECDEcDjctFAAATAGeHoAqtFqtOI5pW0r59ttvj0ajzYoEAABTwTg9AHMjhLBtW/20LGswGGxQHgAAMABPD8DckJvv9Xo0Ts9uhuoBAKCGIHsPQEWSJKEResuyPM+DswcA1BN4egAWRUrZbrcHgwGm3wMAagiy9wDMTRRFKm/PGLMsy3GcNE03KBIAABQBTw/A3KRpGkWRvkcIgew9AKCe4Ms5AFRBStnpdChdnySJ53lI3QMA6gnG6QGoiJSSMvac803LAgAAhcDTAwAAAE0G4/QAAABAk4GnBwAAAJoMPD0AAADQZODpAQAAgCaDVXYAlCW9uJBXV4yxf/nNb/7388/Pfve7p59//uVXXx1+97t/bVnffPXV/W99ix8e0sH2/fv2/fsblRcAABjD3HsACOXF0/NzeX3NGBNPn8rr68+fPTv99NM/Pns29axXbt36c4k3iB8d0YZ19669t0fbzv6+de/eiwNu+gcAALB04OlB8yny4i/2XF3lT/mL11//6vnz6y++UHv+8t6958+ff/7sGWPs77797Yff//4///CHzv5+enHRefw4PT//px/84O+/851nX37569/+9r/G4/+Wkk785muvffvNN19/9dU/ffHFW2+88ert2/9a/J/s7b09lQlwDg5ow7p7V207BwfW3bsLKgQAsFPA04PtRlxeistLNo8Xdw4O7ty+/eXz56/dvs0Y+5/PPvvTl1/+/rPP9AOse/f+dm/v6R/+8H9//OO/ffLJZ9fX1r177vGxc3DgHh/nfW3wy1/2PvrI3tvrP3qkAvTk7ExeXaUXF+LpU3F5mZ6f65ew79//qzfeeP21177z5pv/uL+v5CfJX97d06dF965SBfb9+yo9oFIF1r17zv5+OS0CAJoMPD2oL8qLqw15dfVyzzQXSE6aaQHx31jW9Rdf/Ofvf//GN77xH59+mjmRjqdAmbyvuLxMzs6S01Nyt87BAT86co+PZ3pNFdz7773Xff99wx0lZ2eMsfT8XO+LUDSvJMnH7nQWKSG9uHhR4U23Rl5d6T2JDEgVALDLwNODzVDBiyt39dI/3cSv5KioKgqOyYnqzk+5UsYYPzzUZ8zJ6+vo5CQ9P49OTuTVlXXvHj885EdHdNhc90XBvXNw0P/ggzIhNYmqJM/cOz86onjd2d+39/ZKxuhKpUzrHzAtVVCU7WCUCbhRr54qUIkKpAoA2Drg6cHyWcSLK++iPPHUSezkIEs69SLnlF5cRCcnmfCdHx4uOD8uOTvrfPihePrUENyboYQ//ZXX18npqSpSSQhSy+JT+VRXQO8flEwVqAwK07pfenthpiEAdQCeHswHeVam5ZCX7sWnXpH8UN7zkdub6dT12pYVvpuv0nvyZK7g3kwmXaEH5RkNrG51n5rYyL4+lKD6ByVTBWqDaV0BLEoEYHXA04OX5L04u0n5FsV2C3rxqQKYnbrKZpOTLllzJny39/Zo6N09Pp5Lwrmg4F5eXfkPH/oPH66i/pc9gK83ED86ohV9c6X9l4V6kNjXUwUlZxqqVAEWJQKwFODpd4UKXlzFYcrgLuLF86ip6RQXLsupK6ijkJydJWdn5FfU5Pm1hY/y+rrz+HF0csKPjvqPHq36uirtn1epGtGgtqvPFDyVKtCfTD1VoN9FhpkzDZEqAIDB0zeDpXhxZR9XMeUq49QzaV4VgC5l7Dm9uHjh4E9P2brCdzPRyUnnww8ZYysK7g1kJjTowbTenVpp2n9Z6KkCLEoEoDzw9FuAGhNVG7Xy4hkyYaXZqS8rsqxD+G5GD+4HP/nJZuNplfYvWuhPCf9qqZSaYF6UyBZLFdQnIwJAGeDpN0yRF2cFlmizXjwDuXO1Ojy/VJ18xurSxeLyMjo50cN3mjlP/YnlXmspqOC+/+jRBnMMedRC/3z/bOZC/21nkUWJrCBVgEWJoFbA06+Qeb0406yGih7IZGzcXsx06pnvz6w0ko5OTmiGHcnwYmnc0dFWmFRxedn58MPk9NQ9Pu5/8EFtvaYh7c+0hf5l1js0iZUuSmxeLwrUBHj6iizRi7OazSXOLOjarFPXpVKr49g2hO9mek+e9J48YfUL7s3o3/fNL/TXUzi1eqQ3wtRUARYlgo0ATz+FCv8QZVu8eIbKH5VbG5nwnYbetyV8N6OCe+/dd7uuu439FWb8vq9a6L/mTuHWMXNRYslUgb4oEd8vAjo75+l3x4tnWMpH5daD+vI8he/mfy2z7fSePAmiKPOvcbaaVXzfFyiwKBFUoFGevoIXz/9DlMyn1Ncl+zJZ7kfl1gbNnK/wr2W2Hf1f4/gPH27pU2fGsNA/833f7X3v6sayFiVO/X5RfewGKMPWePpq/5y0eV48w+o+KrceVPhOA8DNDt/NTP2/tw2mDt/3BQosSmw2W+DpWz//ef4h2wUvPpPw4487jx/Tdv2d+lRu/ehHbJfCdzMU3Mvr61Gvt2lZNkPRQn/v3Xf7jx5tVDTwgsxMgnyqwDAMGv/0p2uREWTZAk8ffvyxdffurnnxMlBAvNXznEl+RGw64vISClGoIbntfch3GX1RImPMe+edjYqzu2yBpwcAAABAZV7ZtAC7SJqmUspNS7ErQNsAgB1nwzG9lDJNU8aYbdu2bUspLctijCVJoo5xHId2KvRSBed8xcIujVar5fv+UgQWQgghmKalzB5dV5ZlOY6jn57RZP6ANaPkmSrJVJ9NT46hziVqm1BP6TpZ80XpKdJ1S01D7WIuVZUkSRKGoW3baZpyzn3fX6nMRQ9zSWm3GmVIM8+5rpOiN6XICKv97OtGuIw+zYbFUFp0UVbi9Z9pzaZqaRceD8bYnQ1eOwzDJEk455ZlhWGYpqnjON1uVwiRJEkURa7rMsaSJBFC+L6v3lu9lCBTsrE7mRPXdc3OqTxkTOne6S+pK01T3/dt287oKggC3e11Oh1djUKIwWCwFMEqIISIoogxRq93GIaMsW63q972MAxpO01T27bV/m63a6h2idom2u12HMdLrLCGF03TNEmSNE3jOLYsi146Unu/3zeXUg1CiDAM1/Y4SSnJQKtnQ1nqMtJuNWQ8OedSyna7bTCVaZp2u13dgRUZYXU6u1Gp0tVMfRragjCYnaKLslmv/8yLFmmp8Y/HCyYbIo5jz/P0PYPBwPd99ZNzrrbH47H+M1M6mUz0E3cN3/dHo5G+p9/vx3GsfmZ05bru1CLP84bD4crELEUcx7rkw+FQf0hUK/u+rw5bf9Nn9NnUi8Zx7Pu+rl5d7eZSdcDapC0SQxfGIO32MhgM9HeETKVuEAyGdKYRnkwm/X5/MBjoRmNSWp9FSp5pdqZetOTrP/WiZi01+PFQbCymj6IoE4q5rlsUlxsSKdQJ3ZaAPggCSh9letaMsTRNgyBgjFmWRTGoOVRVeJ4XhqF+cJIkhlhKT4gpGSjLOle2KggCIQTl+mzb1gVot9uUDaMYnUorJJ8pfUdNzBjzPC9/zNSdSsIibZslLGoLIUSn00nTtNVq0ZGWZemqpgMYY3EcR1FElXPOOeedTkdFS1EUhWGofqpze70ejbyQbJxzSpWbL0oCh2EohKBQRt1LkTwGpelwzqMootxm+VIpZafToWeD9J+50yJpFxe48r1sL1EU6Q+DZVn9fj9jEPRS/UUoY4SFEN1uV0oZhqHeCovoc6bZmXrReV9/nZlaaurj8ZJNdTFmxiglY/qNBFgLMrXD6DjOeDym7eFwONd96QcPh8Nutzu1dDwed7vdfC94NBpluvZlUNJOJpNut9vv9/VSepeUSCXrz8T0tCdzO5M5e9xFBxskNLdFmUfX930ltrq6oR56wtVFx+Ox67qGxIwOSZgRWG+dInnMUFuQJLQnE9MbSifFMf1MaSsLPFWMktJuKePxeOqbpe/MBPFFKb2pjEYj1Yh6nSX1aVZykdkpumjJavOlM7XU1MdDp75z74UQwQ3tdjvT91SlmxJv6dCYOm07jjPXEJHruhT9MMYyXW+m6arX66nZLjqdTqdk/kDHsiwaGxNCUOiplzqOo8TIl87FimbOGyRcpC1UDWoCWpmEUxiGvu+r0NayLJpmUeZaNByuznUcR38eqsmjoCgwU1vJ0srSLiKwgQrS1pw0TaemyvRXhrJBrVbrwYMH5mxfnsx0qMxbvKA+i8yO+aIVKKMl1sTHQ2eTM/IU5LCllEIINe3Isix6wym/lzlFpYsb4+wpm0Qpccuy5kpXuq4bBIHrupRLzzzWmdR6FEVBEKg9NEdPJXtLehfK0KprqQT7Klh/Pm2RtiDmPYVmUOp7yusz3+Ku6/a+/pW9RbLfvu+32+0id2suzVNGWraYwAbmlbbmUM45vz8zQkdGNYqiqR19YqoRpvl6tC2ljKIo85RW1qfB7My86LyU0RLRsMdDZ2OeXtcyeZ0kSTJLwspoXD+3QmBaE6h3qR5oKWWr1RoOhyVPVy8MTaM1H+y6Lk1rZzfrSdQpRcN7eTqdjpq8ynJtt0QWf8/nZcG2qIZt25XHCPM5j6IgpjKe5+WdccnSDGuQ1sxc0tYfKWVmBWaSJFP1SYGy3ss3G+E0TT3P098+WrCaqbaCPg1mp+RF56W8lhr2eCg2lr33PI+m3gB2M0dJ/axg+Mh/l/H0SZKQh6YJL9W6R/mpPRUqmQmtq1lzTD+zLWjNsfpZbXAhozHP8zLZqYwYhotyzvVzpZS9Xk/Pfy4OrU0qulNzaf7gVUs7U4Dy0tafzJND+ixyja7rqu9tsFlGOD8OyDnPd+jn1afZ7JS86LyU11LDHg/FJr+cQ3NrlR2nBCbnXE02Jl+SmTidKSWklJzzmsf0NOOaMaZmHTPG+v0+jQqTV1PReYX5xm+//bbruvrjO1VXap55kiSdTkf3o0KI0WhU8l5Up5hWqYZh6LouzZhtt9vUN1cjLPQCmxtICNFut9mNc81P6WeMhWFIU2TVtO2iLLdB22YJZ7YFzcxXvSWqlop6vR4txlVS6RIGQUCBhZTStu1er6frhDpq1BxUbX45wNSLZs7V10yb5SnTFrZt08iulPLBgweDwYDeUHNpp9Mhc0kiZebeF0m7iMDUpkxrbrUe2iztzJrrj67PzKdH1Ouv1Eja8H2feldFRjgIAtrveZ5KBiRJIqX88Y9//Ktf/YoV69PQFowxg9kxXNT3fVpkVPT6my9q1lKzHw9i89+9py8fzfzS2S5Q9KGrekLSGhZAbjXmttDXj82VgKGn3XAWhS9TDzBfdBsfHrYl0m4F1POu9jLujhFeREtbzeY9PQAAAABWR31X2QEAAABgceDpAQAAgCYDTw8AAAA0GXh6AAAAoMnU4ht5BuT1dXp+zg8PNy0IAACAKkQnJ8nZ2b9/8slXz5/zoyP3+NjZ39+0ULtF3efeJ2dnrZ/9bPKLX2xaEAAAAGURl5fJ2VlyehqdnDDG7L29f/je927fuqV+8sNDfnjoHh9vWtKdoO4xPQAAgG0hvbiITk6S09P0/Jwx5hwc+O+9lwniKcRPzs7CX/+aMeYeH/OjI354aN+/vyGpmw88PQAAgIVQzls8fcoYc4+PvXffLXLe7vExhfKqW9B5/Jgx5hwcILe/IpC9BwAAMDf5/HzlhPwSqwJTgacHAABQlnx+frmBeD49gNz+4iB7DwAAYAZz5ecXAbn9VYCYHgAAwBRqklSviRhbDTw9AACAl6w6P78IyO1XA9l7AAAA68vPLwJy+9VATA8AADtKAxLjDbiFNQBPDwAAu0Wd8/OLgNx+EcjeAwDATrAV+flFQG6/CMT0AADQWHY8ub3jt6+ApwcAgKbR1Pz8Iuxybh/ZewAAaAiNz88vwi7n9hHTAwDAFoMEdWV2R3Xw9AAAsH0gP79cmp3bR/YeAAC2BuTnV4Q5t++9885Wa7junt6+f99/771NSwEAALUg/PhjcXnJDw/5++83L8lcB5z9fWd/n73/vsrt9z76yLp713/4cNOiVafu2XsAAAAKeX1t3b27aSl2jm1XOzw9AAAA0GRe2bQA85GmqZRy01IAAAAAW8Ot8XhsWVZmbxiGQggpped5juOs4sJhGKZpOm/9rVbL933O+eICCCGEELZt27ZNe5IkYYxZluU4jrlU/SQcx8nrEACw1Ugp0zRljJEdkFLW9jWvZrENVs6yrDUYQKXhRSpZIkmSJEnium6RDlfhGdfTCneCIOj3+5m9nucxxoIgWF0A7Xlehfpd11X3vCBpmiZJkqZpHMekUPpp23a/3zeX0s8oilzXZYwlSSKE8H1/Rb0iAMCaCcMwSRLOuWVZFJY4jtPtdjct13SqWWyDleOcr8EAUj0kCdU81+lLh2JIgw5X4RnX1Aqu604K8H0/juOi0sVZdf0ziePY933f96eKZC6dTCacc7U9Ho/1nwCA7SWOY8/z9D2DwUA3BfWkgkU1WLm1GcB+vz8YDAyeaJ3EcTxTh0v3XGtohTuc8zAMqatSkiAIKINhWZZt25l+bpqmQRAwxqiUMaYOiKIoiiI6kboh5asNgoDyPN1uN9NnEUJ0Oh3GWBzHdAnGGOe8zE1xzqMoogzJvKU6Kp0CANh2oijK2B/XddWgoW7HLMvqdrsqa9putynXSlaI7JieUzWca7ZjZvO4CAYrtx4DKITodrtSyrwnMngTQxEr1jOdwm6yCI7jkJ4zbkW5G8uyyuQqpJTtdpsx5nkeuTaqoXweaOWtMJlMMr3Xor6D3nFQ291ut9/v66WO46gDhsOh6mIMBgP9Qv1+33EcvX5ztWaRJpMJ59z3/W63Sz/L9Lmo+zYej1V3MhPTG0oniOkBaCiGdzljx4bDYSYYtSxL2a7hcKgfPPPcSbEdW9A8FmGwcusxgKPRSAWseU9U5E3MRWY9c86HwyHdDh02Ho/V8XEcc851tXPOR6ORLtVUPY9Go4z8ruvqrWZgDa3wYu69EKJMv0N1HKSUNCpAk9f0Utu21TQBx3HU0EsURfowjOd5mekD5mrLYNu27/u0XX7WHvWDqGc3V6kQIrih3W7XdgwPALAsMnbMcRwKufQ9KjDN2LGZ5xJT7dji5tGAwcqt2gCqYWZVp15a5E3MRWY9O46jAl+6NA2BZ46nbeq39Xq9mTdCAbeqJ0kS27bnmmO40la402q1pJRRFKlny4yUstPpUEqEMUYJCv2Afr8fhiHlmizLUg99fgqDfuLMassw1xiEju/77Xa7qHNQVGpZFu2k7FC1SwMA6gzle6WUQog4jpMkabVamWPyY5FTKXlu3o4txTyaMdjAlRpAmvlI23lPVORNzEWLtBH7umNijNm2XbJf5ft+r9ejTkYYhhUmGK6uFe7EccxuVq+VEaXT6ejjFrQsQZWSO1dVSSlbrdZwOCQ5MlXpvt9c7RrwPM/QcZtaqlQMAGgSurEi66ksEud8MBhUq7byuesxjwYbuCIDSAutddejeyKDNzEUscXaiKTSuwXlV1eqsF5KOW9Ar1hRK7zI3tNs/jJyZMb8M/mENE3DMNQPVtu2betFaZrq55qrXQOccyll0doJcykAoEl4nkeT4/K4rpuxtrQeuky1lc9dj3k0WLkVGcD8FDzdExm8iaGILdZGjLEkSfQ7DYKgfLaYwvryOfI8K2qFW5PJJAgCujeaoRBFESmRsiKkxH6/Tx2WKIqSJFFJJJq677qu6vmGYajOEkLoc+BpGaLeKkmS+L7vuq65WrNIvV6PVhmqlyE/Pz+PEIJmS9q2TR1AKeWDBw8GgwHnfGZpp9NRVyxzOQDAFkGTt9Vs5zRN1Te7er0ezdxmN8ElTe2mCdgUp5LhCoKAnJlKqxady4x2bBHzaMBg5WzbXrUBDIKANKzPV9c9kcGbmB2NQc90UVIdXY5u9q233qL5/+SkVdPrNZfUM42zlB/OWE8rVPnuPX3YyDChX335KJ9VyH8PqHy1AACwTujz21PtFYWe1b7sVuHcnTWPBm9iKCIWaSNq+grnBkHg+3611P3qwH+4AQAAAJaAECIMwxpO0K77/6cHAAAAak673aYFGpZlZZYO1gHE9AAAAECT2bL/WgsAAACAuYCnBwAAAJoMPD0AAADQZODpAQAAgCYDTw8AAAA0mf8HGowbgSSfoSsAAAAASUVORK5CYII=",
      "text/plain": [
       "Tree('S', [Tree('GPE', [('Alasdair', 'NNP')]), ('is', 'VBZ'), ('an', 'DT'), ('instructor', 'NN'), ('for', 'IN'), Tree('ORGANIZATION', [('General', 'NNP'), ('Assembly', 'NNP')])])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = nltk.ne_chunk(tagged)\n",
    "\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPE] Alasdair\n",
      "[ORGANIZATION] General Assembly\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Alasdair is an instructor for General Assembly'):\n",
    "    print('[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "\n",
    "How: \n",
    "    - TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "    - IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'hates', 'likes', 'sports', 'trees']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each row represents a sentence\n",
    "# Each column represents a word\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'hates', 'likes', 'sports', 'trees']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 1.0, 'hates': 1.6931471805599454, 'likes': 1.2876820724517808, 'sports': 1.2876820724517808, 'trees': 1.6931471805599454}\n"
     ]
    }
   ],
   "source": [
    "# the IDF of each word\n",
    "idf = tfidf.idf_\n",
    "print(dict(list(zip(tfidf.get_feature_names(), idf))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###############\n",
    "#### Exercise #####\n",
    "###############\n",
    "\n",
    "\n",
    "for each sentence in sample, find the most \"interesting words\" by ordering their tfidf in ascending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bob': 1.0,\n",
       " 'hates': 1.6931471805599454,\n",
       " 'likes': 1.2876820724517808,\n",
       " 'sports': 1.2876820724517808,\n",
       " 'trees': 1.6931471805599454}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleTFIDF = dict(list(zip(tfidf.get_feature_names(), idf)))\n",
    "sampleTFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'likes', 'sports', 'hates', 'trees']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sampleTFIDF, key=sampleTFIDF.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Time Demonstrations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA - Latent Dirichlet Allocation\n",
    "\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "\n",
    "Why:   Much quicker than manually creating and identifying topic clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda\n",
      "  Downloading lda-1.0.5-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (387kB)\n",
      "\u001b[K    100% |████████████████████████████████| 389kB 800kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.6.1 in /anaconda/lib/python3.6/site-packages (from lda)\n",
      "Collecting pbr>=0.6 (from lda)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 1.4MB/s a 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pbr, lda\n",
      "Successfully installed lda-1.0.5 pbr-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science is a \"concept to unify statistics, data analysis and their related methods\" in order to \"understand and analyze actual phenomena\" with data.',\n",
       " '[3] It employs techniques and theories drawn from many fields within the broad areas of mathematics, statistics, information science, and computer science, in particular from the subdomains of machine learning, classification, cluster analysis, data mining, databases, and visualization.',\n",
       " 'Turing award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.',\n",
       " '[4][5] When Harvard Business Review called it \"The Sexiest Job of the 21st Century\" [6] the term became a buzzword, and is now often applied to business analytics,[7] or even arbitrary use of data, or used as a sexed-up term for statistics.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 45\n",
      "INFO:lda:vocab_size: 1749\n",
      "INFO:lda:n_words: 2182\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -25173\n",
      "INFO:lda:<10> log likelihood: -21490\n",
      "INFO:lda:<20> log likelihood: -21169\n",
      "INFO:lda:<30> log likelihood: -20862\n",
      "INFO:lda:<40> log likelihood: -20758\n",
      "INFO:lda:<50> log likelihood: -20722\n",
      "INFO:lda:<60> log likelihood: -20650\n",
      "INFO:lda:<70> log likelihood: -20616\n",
      "INFO:lda:<80> log likelihood: -20698\n",
      "INFO:lda:<90> log likelihood: -20591\n",
      "INFO:lda:<100> log likelihood: -20671\n",
      "INFO:lda:<110> log likelihood: -20533\n",
      "INFO:lda:<120> log likelihood: -20327\n",
      "INFO:lda:<130> log likelihood: -20451\n",
      "INFO:lda:<140> log likelihood: -20430\n",
      "INFO:lda:<150> log likelihood: -20405\n",
      "INFO:lda:<160> log likelihood: -20645\n",
      "INFO:lda:<170> log likelihood: -20475\n",
      "INFO:lda:<180> log likelihood: -20341\n",
      "INFO:lda:<190> log likelihood: -20409\n",
      "INFO:lda:<200> log likelihood: -20319\n",
      "INFO:lda:<210> log likelihood: -20418\n",
      "INFO:lda:<220> log likelihood: -20527\n",
      "INFO:lda:<230> log likelihood: -20437\n",
      "INFO:lda:<240> log likelihood: -20399\n",
      "INFO:lda:<250> log likelihood: -20316\n",
      "INFO:lda:<260> log likelihood: -20422\n",
      "INFO:lda:<270> log likelihood: -20209\n",
      "INFO:lda:<280> log likelihood: -20429\n",
      "INFO:lda:<290> log likelihood: -20274\n",
      "INFO:lda:<300> log likelihood: -20345\n",
      "INFO:lda:<310> log likelihood: -20383\n",
      "INFO:lda:<320> log likelihood: -20372\n",
      "INFO:lda:<330> log likelihood: -20375\n",
      "INFO:lda:<340> log likelihood: -20346\n",
      "INFO:lda:<350> log likelihood: -20499\n",
      "INFO:lda:<360> log likelihood: -20333\n",
      "INFO:lda:<370> log likelihood: -20384\n",
      "INFO:lda:<380> log likelihood: -20347\n",
      "INFO:lda:<390> log likelihood: -20476\n",
      "INFO:lda:<400> log likelihood: -20469\n",
      "INFO:lda:<410> log likelihood: -20394\n",
      "INFO:lda:<420> log likelihood: -20351\n",
      "INFO:lda:<430> log likelihood: -20387\n",
      "INFO:lda:<440> log likelihood: -20340\n",
      "INFO:lda:<450> log likelihood: -20248\n",
      "INFO:lda:<460> log likelihood: -20260\n",
      "INFO:lda:<470> log likelihood: -20315\n",
      "INFO:lda:<480> log likelihood: -20544\n",
      "INFO:lda:<490> log likelihood: -20467\n",
      "INFO:lda:<499> log likelihood: -20426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: programs, paid programs, use data, offering, paid, like, time, data sources, collection\n",
      "Topic 1: data, science, data science, journal, university, data science journal, journal data science, degree, publication\n",
      "Topic 2: business, scientist, data scientist, century, 21st, 21st century, use, term, job\n",
      "Topic 3: field, areas, article, applications, extending, computing, computing data, 2001, article data\n",
      "Topic 4: software, present, source, open source software, source software, open, started, current, open source\n",
      "Topic 5: data, scientists, data scientists, information, data driven, insights, new, methods, data collection\n",
      "Topic 6: analytics, conference, international, 2015, launched, conference data, science advanced analytics, science advanced, society\n",
      "Topic 7: statistical, analysis, mining, data mining, section, association, classification, statistician, learning\n",
      "Topic 8: statisticians, digital, produce, work, research, digital data, analysis, enabling research, statisticians normally\n",
      "Topic 9: science, data, data science, statistics, term, term data, computer, term data science, methods\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXAMPLE: Automatically summarize a document\n",
    "\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the happy bastard\\'s quick movie review \\ndamn that y2k bug . \\nit\\'s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \\nlittle do they know the power within . . . \\ngoing for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \\nwe don\\'t know why the crew was really out in the middle of nowhere , we don\\'t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don\\'t know why donald sutherland is stumbling around drunkenly throughout . \\nhere , it\\'s just \" hey , let\\'s chase these people around with some robots \" . \\nthe acting is below average , even from the likes of curtis . \\nyou\\'re more likely to get a kick out of her work in halloween h20 . \\nsutherland is wasted and baldwin , well , he\\'s acting like a baldwin , of course . \\nthe real star here are stan winston\\'s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone\\'s brain . \\nso , if robots and body parts really turn you on , here\\'s your movie . \\notherwise , it\\'s pretty much a sunken ship of a movie . \\n',\n",
       " \"it is movies like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch . \\nbased on the late 1960's television show by the same name , the mod squad tells the tale of three reformed criminals under the employ of the police to go undercover . \\nhowever , things go wrong as evidence gets stolen and they are immediately under suspicion . \\nof course , the ads make it seem like so much more . \\nquick cuts , cool music , claire dane's nice hair and cute outfits , car chases , stuff blowing up , and the like . \\nsounds like a cool movie , does it not ? \\nafter the first fifteen minutes , it quickly becomes apparent that it is not . \\nthe mod squad is certainly a slick looking production , complete with nice hair and costumes , but that simply isn't enough . \\nthe film is best described as a cross between an hour-long cop show and a music video , both stretched out into the span of an hour and a half . \\nand with it comes every single clich ? . \\nit doesn't really matter that the film is based on a television show , as most of the plot elements have been recycled from everything we've already seen . \\nthe characters and acting is nothing spectacular , sometimes even bordering on wooden . \\nclaire danes and omar epps deliver their lines as if they are bored , which really transfers onto the audience . \\nthe only one to escape relatively unscathed is giovanni ribisi , who plays the resident crazy man , ultimately being the only thing worth watching . \\nunfortunately , even he's not enough to save this convoluted mess , as all the characters don't do much apart from occupying screen time . \\nwith the young cast , cool clothes , nice hair , and hip soundtrack , it appears that the film is geared towards the teenage mindset . \\ndespite an american 'r' rating ( which the content does not justify ) , the film is way too juvenile for the older mindset . \\ninformation on the characters is literally spoon-fed to the audience ( would it be that hard to show us instead of telling us ? ) , dialogue is poorly written , and the plot is extremely predictable . \\nthe way the film progresses , you likely won't even care if the heroes are in any jeopardy , because you'll know they aren't . \\nbasing the show on a 1960's television show that nobody remembers is of questionable wisdom , especially when one considers the target audience and the fact that the number of memorable films based on television shows can be counted on one hand ( even one that's missing a finger or two ) . \\nthe number of times that i checked my watch ( six ) is a clear indication that this film is not one of them . \\nit is clear that the film is nothing more than an attempt to cash in on the teenage spending dollar , judging from the rash of really awful teen-flicks that we've been seeing as of late . \\navoid this film at all costs . \\n\",\n",
       " ' \" quest for camelot \" is warner bros . \\' first feature-length , fully-animated attempt to steal clout from disney\\'s cartoon empire , but the mouse has no reason to be worried . \\nthe only other recent challenger to their throne was last fall\\'s promising , if flawed , 20th century fox production \" anastasia , \" but disney\\'s \" hercules , \" with its lively cast and colorful palate , had her beat hands-down when it came time to crown 1997\\'s best piece of animation . \\nthis year , it\\'s no contest , as \" quest for camelot \" is pretty much dead on arrival . \\neven the magic kingdom at its most mediocre -- that\\'d be \" pocahontas \" for those of you keeping score -- isn\\'t nearly as dull as this . \\nthe story revolves around the adventures of free-spirited kayley ( voiced by jessalyn gilsig ) , the early-teen daughter of a belated knight from king arthur\\'s round table . \\nkayley\\'s only dream is to follow in her father\\'s footsteps , and she gets her chance when evil warlord ruber ( gary oldman ) , an ex-round table member-gone-bad , steals arthur\\'s magical sword excalibur and accidentally loses it in a dangerous , booby-trapped forest . \\nwith the help of hunky , blind timberland-dweller garrett ( carey elwes ) and a two-headed dragon ( eric idle and don rickles ) that\\'s always arguing with itself , kayley just might be able to break the medieval sexist mold and prove her worth as a fighter on arthur\\'s side . \\n \" quest for camelot \" is missing pure showmanship , an essential element if it\\'s ever expected to climb to the high ranks of disney . \\nthere\\'s nothing here that differentiates \" quest \" from something you\\'d see on any given saturday morning cartoon -- subpar animation , instantly forgettable songs , poorly-integrated computerized footage . \\n ( compare kayley and garrett\\'s run-in with the angry ogre to herc\\'s battle with the hydra . \\ni rest my case . ) \\neven the characters stink -- none of them are remotely interesting , so much that the film becomes a race to see which one can out-bland the others . \\nin the end , it\\'s a tie -- they all win . \\nthat dragon\\'s comedy shtick is awfully cloying , but at least it shows signs of a pulse . \\nat least fans of the early-\\'90s tgif television line-up will be thrilled to find jaleel \" urkel \" white and bronson \" balki \" pinchot sharing the same footage . \\na few scenes are nicely realized ( though i\\'m at a loss to recall enough to be specific ) , and the actors providing the voice talent are enthusiastic ( though most are paired up with singers who don\\'t sound a thing like them for their big musical moments -- jane seymour and celine dion ? ? ? ) . \\nbut one must strain through too much of this mess to find the good . \\naside from the fact that children will probably be as bored watching this as adults , \" quest for camelot \" \\'s most grievous error is its complete lack of personality . \\nand personality , we learn from this mess , goes a very long way . \\n',\n",
       " 'synopsis : a mentally unstable man undergoing psychotherapy saves a boy from a potentially fatal accident and then falls in love with the boy\\'s mother , a fledgling restauranteur . \\nunsuccessfully attempting to gain the woman\\'s favor , he takes pictures of her and kills a number of people in his way . \\ncomments : stalked is yet another in a seemingly endless string of spurned-psychos-getting-their-revenge type movies which are a stable category in the 1990s film industry , both theatrical and direct-to-video . \\ntheir proliferation may be due in part to the fact that they\\'re typically inexpensive to produce ( no special effects , no big name stars ) and serve as vehicles to flash nudity ( allowing them to frequent late-night cable television ) . \\nstalked wavers slightly from the norm in one respect : the psycho never actually has an affair ; on the contrary , he\\'s rejected rather quickly ( the psycho typically is an ex-lover , ex-wife , or ex-husband ) . \\nother than that , stalked is just another redundant entry doomed to collect dust on video shelves and viewed after midnight on cable . \\nstalked does not provide much suspense , though that is what it sets out to do . \\ninterspersed throughout the opening credits , for instance , a serious-sounding narrator spouts statistics about stalkers and ponders what may cause a man to stalk ( it\\'s implicitly implied that all stalkers are men ) while pictures of a boy are shown on the screen . \\nafter these credits , a snapshot of actor jay underwood appears . \\nthe narrator states that \" this is the story of daryl gleason \" and tells the audience that he is the stalker . \\nof course , really , this is the story of restauranteur brooke daniels . \\nif the movie was meant to be about daryl , then it should have been called stalker not stalked . \\nokay . so we know who the stalker is even before the movie starts ; no guesswork required . \\nstalked proceeds , then , as it begins : obvious , obvious , obvious . \\nthe opening sequence , contrived quite a bit , brings daryl and brooke ( the victim ) together . \\ndaryl obsesses over brooke , follows her around , and tries to woo her . \\nultimately rejected by her , his plans become more and more desperate and elaborate . \\nthese plans include the all-time , psycho-in-love , cliche : the murdered pet . \\nfor some reason , this genre\\'s films require a dead pet to be found by the victim stalked . \\nstalked is no exception ( it\\'s a cat this time -- found in the shower ) . \\nevents like these lead to the inevitable showdown between stalker and stalked , where only one survives ( guess who it invariably always is and you\\'ll guess the conclusion to this turkey ) . \\nstalked\\'s cast is uniformly adequate : not anything to write home about but also not all that bad either . \\njay underwood , as the stalker , turns toward melodrama a bit too much . \\nhe overdoes it , in other words , but he still manages to be creepy enough to pass as the type of stalker the story demands . \\nmaryam d\\'abo , about the only actor close to being a star here ( she played the bond chick in the living daylights ) , is equally adequate as the \" stalked \" of the title , even though she seems too ditzy at times to be a strong , independent business-owner . \\nbrooke ( d\\'abo ) needs to be ditzy , however , for the plot to proceed . \\ntoward the end , for example , brooke has her suspicions about daryl . \\nto ensure he won\\'t use it as another excuse to see her , brooke decides to return a toolbox he had left at her place to his house . \\ndoes she just leave the toolbox at the door when no one answers ? \\nof course not . \\nshe tries the door , opens it , and wanders around the house . \\nwhen daryl returns , he enters the house , of course , so our heroine is in danger . \\nsomehow , even though her car is parked at the front of the house , right by the front door , daryl is oblivious to her presence inside . \\nthe whole episode places an incredible strain on the audience\\'s suspension of disbelief and questions the validity of either character\\'s intelligence . \\nstalked receives two stars because , even though it is highly derivative and somewhat boring , it is not so bad that it cannot be watched . \\nrated r mostly for several murder scenes and brief nudity in a strip bar , it is not as offensive as many other thrillers in this genre are . \\nif you\\'re in the mood for a good suspense film , though , stake out something else . \\n']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOWEST:\n",
      "\n",
      "we'll have a hit on our hands ! \"\n",
      "obviously the problem is that without a good script , any director will fail in the end .\n",
      "the usual things happen like clockwork .\n",
      "\n",
      "HIGHEST:\n",
      "\n",
      "keanu reeves stars as shane falco , a has-been football college player looking for redemption .\n",
      "throw in that hunk of a guy keanu reeves and a cast of wacky characters and poof !\n",
      "gene hackman dons a fedora like tom landry and speaks with gusto like a certain coach in hoosiers .\n"
     ]
    }
   ],
   "source": [
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    \n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TFIDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print('\\nLOWEST:\\n')\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print(sent_score[1])\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print('\\nHIGHEST:\\n')\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print(sent_score[1])\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.12.0-py2.py3-none-any.whl (631kB)\n",
      "\u001b[K    100% |████████████████████████████████| 634kB 694kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /anaconda/lib/python3.6/site-packages (from textblob)\n",
      "Requirement already satisfied: six in /anaconda/lib/python3.6/site-packages (from nltk>=3.1->textblob)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.12.0\n"
     ]
    }
   ],
   "source": [
    "# TextBlob Demo: \"Simplified Text Processing\"\n",
    "# Installation: pip install textblob\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['liam', 'sinan', 'general assembly'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify words and noun phrases\n",
    "blob = TextBlob('Liam and Sinan are instructors for General Assembly')\n",
    "blob.words\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9, -0.26923076923076916]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print(TextBlob('Ian and Alasdair are instructors for General Assembly in Sydney').sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Put', 'away', 'the', 'dish']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Puts', 'aways', 'thes', 'dishess']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.pluralize() for word in blob.words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), ('parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tip laterally',\n",
       " 'enclose with a bank',\n",
       " 'do business with a bank or keep an account at a bank',\n",
       " 'act as the banker in a game or in gambling',\n",
       " 'be in the banking business',\n",
       " 'put into a bank account',\n",
       " 'cover with ashes so to control the rate of burning',\n",
       " 'have confidence or faith in']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "blob.translate(to='es')\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expressions - Regex - References\n",
    "\n",
    "This is the python module for regular expressions: https://docs.python.org/2/library/re.html\n",
    "\n",
    "Here is a google page for explaining regular expression patterns: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "And here is a convenient tool for testing regular expressions: https://regex101.com/#python\n",
    "\n",
    "Have a read of these and play around with regular expressions below and in the regex101 tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
